---
title: "Analyse thématique des avis des voyageurs sur AirBnB"
author: "CB"
date: "18 avril 2019"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
bibliography: AirbnbInside.bib
---
<style type="text/css">

body, td {
   font-size: 14px;
}
code.r{
  font-size: 10px;
}
h1{
  font-size: 24px;
}
h2{
  font-size: 18px;
}
pre {
  font-size: 11px
}
</style>

<center>

![Paris Invader, rue Mouffetard - source perso](invader_mouff.jpg)

</center>

La proposition première de Airbnb est celle de expérience du voyage. Non pas cette sagesse qui s'accumule au travers des rencontres, mais le sentiment gratifiant de vivre une chose exceptionnelle, unique, authentique, pleine de rencontres et de découvertes. Trouve-t-on l'écho de cette expérience dans le contenu considérable des avis produits par les voyageurs? A quels aspects de l'expérience se réfèrent-il ? 

# 1. Introduction

Dans cette note de recherche nous analysons 1 000 000 d'avis de voyageurs relatifs aux 57000 offres disponibles en février 2019. Ces données proviennent du site [Inside Airbnb](http://insideairbnb.com/get-the-data.html). L'objectif est de construire un modèle de topic sur la base de différentes techniques d'annotation. Elles sont utilisées pour filtrer les éléments du corpus et donner plus de lisibilité aux solutions obtenues. On y utilise une technique d'identification des dépendances syntaxiques pour affiner l'analyse. 

Le "pipe" de traitement consiste à

 * 1) Détecter les langues employées dans le corpus
 * 2) Pour chacune des langues, annoter les tokens (les termes) par une analyse des éléments du discours (POS)
 * 3) Attribuer un genre aux prénoms. 
 * 4) Réduire le corpus aux : noms communs, adjectifs et adverbes, ainsi qu'aux verbes. On élimine ainsi noms de lieu, ponctuations etc.
 * 5) Construire le modèle de topics (sujets) par une méthode LDA
 * 6) Le visualiser avec LDAvis
 * 7) Annoter le corpus avec les dépendances et examiner la relation des noms communs à de leurs adjectifs.


## 1.1 Initialisation

pour l'identification des langues on emploie `texcat`, l'annotation est réalisée avec `cleanNLP`, on emploie `text2vec` pour le LDA, pour la représentation des dépendances `packcircles` sera utile. Pour le détail du code on consultera le [repo]().

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, include=TRUE,message=FALSE,warning=FALSE,cache=TRUE)

library(readr)
library(tidyverse)
library(lubridate)
library(textcat)
library(cleanNLP)
library(packcircles)
library(knitr)
library(kableExtra)
library(gridExtra)
library(text2vec)
library(reshape2)
```

la lecture du fichier ne pose aucune difficulté, en voici les 7 premiers éléments pour avoir une idée de sa structure.

```{r data01, include=TRUE, message=TRUE}
library(readr)
reviews <- read_csv("reviews.csv")
head(reviews,7)
```

## 1.2 Echantillonner les données pour mieux calibrer

Pour mettre au point les analyses, on s'appuie sur un échantillon restreint, on attaque que dans un temps final  les 1 097 737 commentaires qui demanderons plusieurs heures de traitement. Certaines opérations ( détection des langues et annotation) demande un très long temps de calcul. Dans cette version, on se contente de 30 000 commentaires. 

Pour se donner une idée plus précise de leur nature, on examine leur distribution dans le temps. Celle-ci illustre la croissance de la plateforme, mais plus encore, dans la mesure où ces commentaires se rapportent à l'offre disponible le 4 février 2019, l'ancienneté des offres dans l'inventaire de Airbnb. 


```{r sample}
#on échantillonne pour tester le code
df<-reviews %>% sample_n(30000)
#un format qu'aime lubridate
df$date<-as.POSIXct(df$date)

g01<-df %>% 
  ggplot(aes(date)) + 
  geom_freqpoly(binwidth = 86400) + labs(title = "Nombre d'avis déposés par jour",caption = "AirBnb Inside",x="temps",y="nb de reviews") +theme_minimal()
g01
```

En réalité il s'agit moins d'une série chronologique que d'une pyramide des ages. En effet les données résultent d'un processus marqué d'abord par la disparition des logements de l'inventaire de Airnb - le faible nombre des avis en 2010 et 2011, reflètent le fait que très peu d'offres proposées à cette période le sont encore à la date de l'étude, mais aussi par le fait qu'à chaque période de nouvelles offres génèrent de nouveaux avis. 

Une autre représentation est préférable, en pyramide des âge de l'offre. On s'aperçoit que la valeur centrale de l'age des annonces est en moyenne de 2,95 années, et une médiane de 2.8 années. La moitiés des avis se rapportent à des offres proposées depuis plus de 3 ans. Il y a une véritable fidélité des propriétaires de logement.

```{r pyr, include=TRUE, echo=TRUE}
df$year<-year(df$date)

g02<-df %>% 
  ggplot(aes(year)) + 
  geom_histogram(binwidth = 1,fill="royalblue3") + labs(title = "Pyramide des âges des annonces",caption = "AirBnb Inside",x="années",y="nombre d'avis") +coord_flip()+scale_x_continuous(breaks = c(2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019))+theme_minimal()

g02

```

# 2. Identification des langues

Quand les corpus sont constitués de plusieurs langues, il est donc essentiel de pouvoir les identifier afin de leur appliquer un traitement adéquat et indépendant. Les langues se construisent en elle-mêmes, par leur lexique et leur grammaire. 

## 2.1 avec textcat 

A cette fin on emploie le package `textcat`(@hornik_textcat_2013), qui s'appuie sur la distribution des ngrams propre à chaque langue et qui constituent en quelque sorte leur signature. Le principe est de calculer une distance entre la distribution des ngram du texte cible avec celle qui caractérise les langues. 

![calcul de distance à la langue](distancelangue.jpg)


L'attribution à une langue se fait avec le critère de la plus faible distance de la cible. 

On examinera la fiche [wikipedia](https://en.wikipedia.org/wiki/N-gram) pour plus de détail sur l'utilisation des Ngrams en NLP.


```{r langage01, include=TRUE, echo=TRUE}
#la détection de langue
df$langue<-textcat(df$comments) #attention c'est long - une bonne dizaines de minutes.
```

Incidemment on notera la domination des contenus en anglais (un peu plus de 60%) le français ne comptant que pour 22%, l'espagnol et l'allemand suivent avec moins de 5%. Dans la suite de l'exercise on se concentre sur le corpus anglophone, on laisse le soin au lecteur de reproduire l'analyse sur les autres langues.

On représente celà par une sorte de pyramide des ages composées par les langages. Le second diagramme donne les proportion d'avis par langue au cours de la période : on assiste à une forte diversication des langues. En 2010 les commentaires sont essentiellement en anglais, le français émerge en 2013 pour prendre un tiers des avis produits chaque année. Les autres langues progressent aussi et l'anglais cède du terrain pour ne plus représenter que la moitié des avis.

```{r langage02, include=TRUE, caption="la distribution des avis par pays"}

df$langue<-as.factor(df$langue)
df$langues<-"autres"
df$langues[df$langue=="english"]<-"english"
df$langues[df$langue=="french"]<-"french"
df$langues[df$langue=="german"]<-"german"
df$langues[df$langue=="spanish"]<-"spanish"
df$langues[df$langue=="portuguese"]<-"portuguese"

Table <- with(df, table(langues,year))
Pays<-as.data.frame(Table)

#la fréquence par pays
g<-ggplot(Pays,aes(x=reorder(year,Freq),y=Freq, group=year))+geom_bar(stat="identity",aes(fill=langues),position="stack")+coord_flip()+theme_minimal()+theme(text = element_text(size=10))+scale_fill_brewer( direction = -1)+labs(title = "Fréquence des langues dans le corpus",caption = "AirBnb Inside",x="temps",y="nombre d'avis")
g
gb<-ggplot(Pays,aes(x=year,y=Freq,group=year))+geom_bar(stat="identity",aes(fill=langues),position="fill")+coord_flip()+theme_minimal()+theme(text = element_text(size=10))+scale_fill_brewer( direction = -1)+labs(title = "Proportion des langues dans le corpus",caption = "AirBnb Inside",x="temps",y="% des avis")
gb
#creation des sous corpus
df_fr <-subset(df,langue=="french")
df_en <-subset(df,langue=="english")

```

## 2.2 Les locuteurs

En complément, il peut être utile de s'intéresser aux locuteurs pour lesquels on dispose de peu d'informations sinon leur prénom. On peut en tirer au moins une information sur le genre en matchant avec une [table des prénoms](https://data.world/arunbabu/gender-by-names). 

Il semblerait que les femmes contribuent plus que les hommes, mais parmi les prénoms plus fréquents, on retrouve plutôt des prénoms masculins. 


```{r pseudo , echo = FALSE}
df_en <-subset(df,langue=="english")
gender_refine_csv <- read_csv("gender_refine-csv.csv")%>%mutate(reviewer_name=name)
df_en<-df_en %>% left_join(gender_refine_csv, by ="reviewer_name")

df_en$n<-1

gender<-aggregate(n~gender,data=df_en, FUN="sum")
gender$gender[gender$gender==3]<-"Male and female"
gender$gender[gender$gender==2]<-"other"
gender$gender[gender$gender==1]<-"Male"
gender$gender[gender$gender==0]<-"Female"


g21<-ggplot(gender,aes(x=gender,y=n))+geom_bar(aes(fill=gender),stat="identity")+coord_flip()+theme_minimal()+theme(text = element_text(size=9))+scale_fill_brewer(palette="Accent")+labs(title = "Fréquence des genres",caption = "AirBnb Inside",x="Genre",y="nombre d'avis")
g21

df_en$m<-1
locuteur<-aggregate(m~reviewer_name,data=df_en, FUN="sum")
locuteur<-locuteur %>% filter( m>50) %>% left_join(gender_refine_csv, by ="reviewer_name")
locuteur$gender[locuteur$gender==3]<-"Male and female"
locuteur$gender[locuteur$gender==2]<-"other"
locuteur$gender[locuteur$gender==1]<-"Male"
locuteur$gender[locuteur$gender==0]<-"Female"

g21<-ggplot(locuteur,aes(x=reorder(reviewer_name,m),y=m,group=gender))+geom_bar(aes(fill=gender),stat="identity")+coord_flip()+theme_minimal()+theme(text = element_text(size=8))+scale_fill_brewer(palette="Accent")+labs(title = "Fréquence des genres",caption = "AirBnb Inside",x="Prénom",y="nombre d'avis")
g21
```

 

# 3. La lemmatisation

Avant d'explorer plus avant le contenu des tweets, on va d'abord annoter le corpus en utilisant les ressources de `cleanNLP` et en particulier son outil d'analyse des parties du discours et de lemmatisation qui permettra à la fois d'obtenir les lemmes correspondants à chaque des termes mais aussi à leur forme morphosyntaxique. Nous nous inspirons largement de leur [étude de cas](https://cran.r-project.org/web/packages/cleanNLP/vignettes/case_study.html)

## 3.1 Des éléments du discours


La composition en termes morphosyntaxique n'est pas forcément évidente à interprêter, il faudrait connaitre la distribution dans des corpus de référence. Mais cette annotation est cependant utile pour filtrer le contenu.

```{r lemme00, echo = TRUE}
#library(cleanNLP)
text<-df_en$comments

cnlp_init_udpipe(model_name = "english")
obj <- cnlp_annotate(text, as_strings = TRUE)
head(cnlp_get_token(obj))
```

```{r lemme01, echo = FALSE}

Vocab<-cnlp_get_token(obj)
Table <- with(Vocab, table(upos))
ling<-as.data.frame(Table)
g1<-ggplot(ling,aes(x=reorder(upos,Freq),y=Freq))+geom_bar(stat="identity",fill="darkgreen")+coord_flip()+theme_minimal()+theme(text = element_text(size=9))
g1+labs(title = "Fréquence des catégories morpho-syntaxiques",caption = "AirBnb Inside",x="catégorie",y="nombre d'avis")
```

Nous pouvons désormais travailler sur un corpus linguistiquement plus homogène. On doit remarquer que le caractère cardinal du découpage est aussi fonction de convention. On pourrait imaginer une dectetion des langues plus fine qui prendrait en compte les dialectes. C'est le travail des linguistes que de produire des catégories pertinentes et nous nous fierons à leurs avancées. 


## 3.2 verbes, noms communs et adjectif et adverbes

Le texte à l'état brut comporte de nombreux signes qui amènent peu de sens lexical : des point , des noms de lieux, des déterminants. La stratégie que nous engagons est différentes de cette de tm qui juxtapose : mise en minuscule, suppresion des ponctuactions, des mentions et des ref html, notre stratégie n'élimine pas mais se concentre sur les éléments lexicaux les plus signifiants. (au moins de manières premières, les autres éléments secondaires peuvent révéler des inflexions plus fines, comme l'usage de la ponctuation le permet - Proust l'aura démontré ref  les coutures proust)

Il s'agira simplement des verbes, des noms propres, des adjectifs et des adverbes. On en présente les trois distributions.

 * les noms communs mettent en tête des éléments liés aux logement, à sa localisation et son voisinage, aisi qu'au séjour et à l'accueil.
 * Les qualificatifs sont très positifs et principalement expressifs, proche, propre et confortable en sont les attributs les plus fréquents
 * Les verbes traduisent une intention : recommendation, des mouvements : stay, walk go et des transaction : get, need...
 
 L'expérience reste largement inexprimée si ce n'est pas un jugement très positif qui nous incite à explorer dans une prochaine note les sentiments et émotions associées.

```{r pos01, fig.width=8, fig.height=4.5, caption ="fréquence des termes par POS : Noms communs, adjectifs et adverbes,verbes"}
Vocab1<-subset(Vocab, upos=="NOUN")
Table <- with(Vocab1, table(lemma))
ling<-as.data.frame(Table) %>% filter(Freq>1100)
g2<-ggplot(ling,aes(x=reorder(lemma,Freq),y=Freq))+geom_bar(stat="identity",fill="brown1")+coord_flip()+theme_minimal()+theme_minimal()+ theme(axis.title.x=element_blank())+ theme(axis.title.y=element_blank(),axis.text=element_text(size=8))+labs(title = "Fréquence des lemmes",x="Noms commun",y="nombre d'avis")


Vocab2<-subset(Vocab, upos=="ADJ" | upos=="ADV")
Table <- with(Vocab2, table(lemma))
ling<-as.data.frame(Table) %>% filter(Freq>1200)
g3<-ggplot(ling,aes(x=reorder(lemma,Freq),y=Freq))+geom_bar(stat="identity",fill="brown2")+coord_flip()+theme_minimal()+theme(text = element_text(size=3))+theme_minimal()+ theme(axis.title.x=element_blank())+ theme(axis.title.y=element_blank())+labs(title = " ",x="Adverbe et adjectifs",y="nombre d'avis")


Vocab3<-subset(Vocab, upos=="VERB")
Table <- with(Vocab3, table(lemma))
ling<-as.data.frame(Table) %>% filter(Freq>800)
g4<-ggplot(ling,aes(x=reorder(lemma,Freq),y=Freq))+geom_bar(stat="identity",fill="brown4")+coord_flip()+theme_minimal()+theme(text = element_text(size=2), label = NULL)+theme_minimal()+ theme(axis.title.x=element_blank())+ theme(axis.title.y=element_blank())+labs(title = " ",x="verbes",y="nombre d'avis")

grid.arrange(g2,g3,g4,ncol=3)

```



# 4. Un modèle LDA de topics

pour mieux cerner les sujet on utilise le desormais bien connu modèle LDA avec la solution proposée par [`text2vec`](http://text2vec.org/) et la solution de visualisation interactive LDAvis.

On ne retient qu'une partie du corpus : les adjectifs, les noms communs et les verbes. La méthode nous permet d'ôter nombres et signes de ponctuations comme on le fait avec tm et les autres packages, mais aussi les noms de lieux (noms communs) qui peuvent "fragmenter" les topics en leur donnant une spécificité géographique. On travaille sur les lemmes ce qui permet de réduire le bruit généré par les fautes d'orthographes inévitables dans un corpus dont la langue est largement vernaculaire.

Les résultat se présente sous la forme d'une liste des termes les plus pertinents selon le critère proposé par @sievert_ldavis_2014 pour chacun des sujets identifiés. On opte après essais/erreurs pour un modèle à 8 thèmes qui se regroupent en 4 familles.

En synthèse on trouve :
 
 * Un thème représentant clairement l'*accueil par l'hôte*. (9%)
 * Deux thèmes relatif à l'*environnement*, l'un orienté vers le voisinage accessible au bas de l'immeuble :café, boutiques, restaurants, l'autre à l'interconnexion avec les moyens de transports.(26%)
 * Deux thèmes relatifs aux *éléments matériels* : la facilité d'usage du service, les équipements du logement (le lit en tête). (32%)
 * trois thèmes exprimant la *satisfaction* à l'égard du sejour et l'intention de recommander, un autre manifestant l'enthousiasme, le dernier enfin évoque pleinement l'expérience (36% des tokens).
 
L'expérience n'est pas absente mais pas dominante. L'usage du logement et l'environnement proche sont les thématiques principales des avis de consommateurs.

```{r lda}
tf <- cnlp_get_token(obj) %>%
  filter(upos %in% c("ADJ", "NOUN","VERB")) %>%
  cnlp_utils_tfidf(min_df = 0.05, max_df = 0.95, type = "tf", tf_weight = "raw")
set.seed(67)
lda_model = LDA$new(n_topics = 8, doc_topic_prior = 0.1, topic_word_prior = 0.01)
doc_topic_distr = 
  lda_model$fit_transform(x = tf, n_iter = 1000, 
                          convergence_tol = 0.001, n_check_convergence = 25, 
                          progressbar = FALSE)
#description des topic en fonction d'un degré de pertinence de lamba ( lambda =1 probabilités)

lda_res<-as.data.frame(lda_model$get_top_words(n = 25, lambda = 0.30))
lda_res$rank<-as.numeric(row.names(lda_res))
lda_res<-melt(lda_res,id.vars = c("rank"))


ggplot(lda_res, aes(x=variable, y= rank, group =  value , label = value)) + scale_y_reverse() + geom_text(aes(color=variable,size=8/log(rank)))+theme_minimal()+scale_color_hue()+guides(color=FALSE,size=FALSE)
```


on pourra générer une visualisation dynamique qui facilite l'interprétation avec le package [`LDAvis`](https://ldavis.cpsievert.me/reviews/reviews.html), en activant le code suivant. (on peut voir ici une [démonstration](http://www.kennyshirley.com/LDAvis/) )

```{r lda02, echo=TRUE}

# la visualisation interactive
library(LDAvis)
lda_model$plot()
```

Le résultat est une visualisation interactive qui pour chacun des topics et des mots indique :

 *  sa proximité avec les autres topics via une analyse des similarités, ce qui est représenté sur le panneau de gauche. La surface des cercles est proportionnelle à la frequence des tokens. 
 *  pour un topic donné, le profil est indiqué à droite : les trente principaux termes classés par pertinence, leurs fréquences sont indiquées sur les barres horizontales et comparées à la fréquence du mot dans l'ensemble du corpus.
 * en cliquant sur les mots on peut aussi observer leur distribution dans les différents groupes. 
 

![ldavis06](ldavisHospitalityArrival.jpg)


# 5. Analyse des dépendances

Certains annotateurs peuvent repérer les règles grammaticales et trouver pour un mot cible ses corrélats.  Un nom commun peut être ainsi associé à des adjectifs qui le qualifient et le nuancent.

on choisit la dépendence "amod" :  adjectival modifier - An adjectival modifier of an NP is any adjectival phrase that serves to modify the meaning of the NP.“Sam eats red meat”amod(meat, red). Pour plus de détail il faut consulter le Stanford typed dependencies manual Marie-Catherine de Marneffe and Christopher D. Manning September 2008 Revised for Stanford Parser v.  1.6.2 in February 2010  on en trouvera un exposé ici [universal dependencies]: (http://universaldependencies.org/u/dep/).

Pour mieux comprendre le modèle LDA on peut ainsi être inciter à étudier certains des lemmes contributifs (on ne travaille plus sur le texte brute mais sur un texte filtré). Dans l'exemple suivant, on s'inspire [du code suivant] (https://github.com/yanhann10/opendata_viz/tree/master/refugee) [commenté ici ](https://towardsdatascience.com/analyse-public-discourse-on-refugees-with-cleannlp-9719a29ed898), on représente les qualificatifs de 4 notions qui apparaissent comme étant les plus fréquentes au travers de leur manifestation par l'usage de noms communs : le logement, son environnement, l'hôte et le séjour.


```{r dep01}

dep<-cnlp_get_dependency(obj, get_token = TRUE)
res <- dep %>%filter(relation == "amod")%>%filter(lemma == "apartment"|lemma == "room")

bub<-res$lemma_target %>%table() %>%sort(decreasing = TRUE) %>%head(n = 40)%>% as.data.frame()
# libraries library(packcircles)

# Generate the layout
packing <- circleProgressiveLayout(bub$Freq, sizetype='area')
packing$radius=0.95*packing$radius
data = cbind(bub, packing)
dat.gg <- circleLayoutVertices(packing, npoints=40)
# ---- Custom the color of the dots: proportional to the value:
# First I need to add the 'value' of each group to dat.gg.
# Here I repeat each value 51 times since I create my polygons with 50 lines
#png("~/Dropbox/R_GG/R_GRAPH/#307_Add_space_between_circles.png", height = 480, width=480)
gd1<- ggplot() + 
  geom_polygon(data = dat.gg, aes(x, y, group = id, fill=id), colour = "black", alpha = 0.6) +
  geom_text(data = data, aes(x, y, size=Freq, label = .), color="black") +
  theme_void() +   scale_fill_distiller(palette = "BuPu", direction = -1 ) +
  theme(legend.position="none")+ labs(title="Qualificatifs du lemme 'appartement' et 'room'")+
  coord_equal()

#place

dep<-cnlp_get_dependency(obj, get_token = TRUE)
res <- dep %>%filter(relation == "amod")%>%filter(lemma == "place"|lemma == "location")

bub<-res$lemma_target %>%table() %>%sort(decreasing = TRUE) %>%head(n = 40)%>% as.data.frame()
# libraries library(packcircles)

# Generate the layout
packing <- circleProgressiveLayout(bub$Freq, sizetype='area')
packing$radius=0.95*packing$radius
data = cbind(bub, packing)
dat.gg <- circleLayoutVertices(packing, npoints=40)

gd2<- ggplot() + 
  geom_polygon(data = dat.gg, aes(x, y, group = id, fill=id), colour = "black", alpha = 0.6) +
  geom_text(data = data, aes(x, y, size=Freq, label = .), color="black") +
  theme_void() +   scale_fill_distiller(palette = "PuRd", direction = 1 ) +
  theme(legend.position="none")+ labs(title="Qualificatifs des lemmes 'place' et 'location'")+coord_equal()


grid.arrange(gd1,gd2,ncol=2)
```

```{r dep02}

dep<-cnlp_get_dependency(obj, get_token = TRUE)
res <- dep %>%filter(relation == "amod")%>%filter(lemma == "host")

bub<-res$lemma_target %>%table() %>%sort(decreasing = TRUE) %>%head(n = 40)%>% as.data.frame()
# libraries library(packcircles)

# Generate the layout
packing <- circleProgressiveLayout(bub$Freq, sizetype='area')
packing$radius=0.95*packing$radius
data = cbind(bub, packing)
dat.gg <- circleLayoutVertices(packing, npoints=40)
# ---- Custom the color of the dots: proportional to the value:
# First I need to add the 'value' of each group to dat.gg.
# Here I repeat each value 51 times since I create my polygons with 50 lines
#png("~/Dropbox/R_GG/R_GRAPH/#307_Add_space_between_circles.png", height = 480, width=480)
gd1<- ggplot() + 
  geom_polygon(data = dat.gg, aes(x, y, group = id, fill=id), colour = "black", alpha = 0.6) +
  geom_text(data = data, aes(x, y, size=Freq, label = .), color="black") +
  theme_void() +   scale_fill_distiller(palette = "YlGn", direction = -1 ) +
  theme(legend.position="none")+ labs(title="Qualificatifs du lemme 'host'")+
  coord_equal()

#place

dep<-cnlp_get_dependency(obj, get_token = TRUE)
res <- dep %>%filter(relation == "amod")%>%filter(lemma == "stay")

bub<-res$lemma_target %>%table() %>%sort(decreasing = TRUE) %>%head(n = 40)%>% as.data.frame()
# libraries library(packcircles)

# Generate the layout
packing <- circleProgressiveLayout(bub$Freq, sizetype='area')
packing$radius=0.95*packing$radius
data = cbind(bub, packing)
dat.gg <- circleLayoutVertices(packing, npoints=40)

gd2<- ggplot() + 
  geom_polygon(data = dat.gg, aes(x, y, group = id, fill=id), colour = "black", alpha = 0.6) +
  geom_text(data = data, aes(x, y, size=Freq, label = .), color="black") +
  theme_void() +   scale_fill_distiller(palette = "Blues", direction = 1 ) +
  theme(legend.position="none")+ labs(title="Qualificatifs des lemmes 'stay' ")+coord_equal()


grid.arrange(gd1,gd2,ncol=2)
```

# 6. Conclusion

Pour amener l'étude à son terme, il faudrait naturellement examiner la distributions des sujets d'avis sous l'angle d'une analyse comparative de groupes de commentateurs. 

On aura réussi à partir de l'analyse des langues à leur attribué une origine culturelle de ces commentateurs, on aura aussi déterminer leur genre à partir de l'analyse des prénoms. On pourrait aussi les qualifier en terme d'engagement en dénombrant le nombre des avis qu'ils ont produits. Même si on possède peu d'informations relatives aux auteurs des avis, on peut en identifier certaines à partir de ce qu'ils écrivent.

Reste à systématiser la comparaison : utilise-t-on les même qualificatifs selon qu'on soit un homme, une femme? Selon l'habitude que l'on a du service ? Selon l'origine culturelle? Parle-t-on des mêmes sujets ?

L'objectif de cette note demeure l'exposition de quelques techniques d'analyse et n'a pour but que de stimuler l'imagination. Ses résultats sont des faits que l'on peut questionner grace à l'ampleur du corpus. L'objectif est de fournir un premier outillage pour des recherches mieux problématisées.

# 7. Bibliographie